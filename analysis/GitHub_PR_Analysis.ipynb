{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06b06705",
   "metadata": {},
   "source": [
    "# GitHub PR Analysis - Jeremy Chia\n",
    "## Comprehensive Analysis of 1,346 Pull Requests\n",
    "\n",
    "This notebook provides a deep dive into GitHub contribution patterns, productivity metrics, and collaboration insights based on enhanced PR data from August 2023 to February 2026.\n",
    "\n",
    "### üìä Dataset Overview\n",
    "- **Total PRs**: 1,346 pull requests\n",
    "- **Time Period**: August 2023 - February 2026  \n",
    "- **Enhanced Fields**: Descriptions, reviews, comments, reactions, timelines, code metrics\n",
    "- **Data Source**: GitHub CLI with comprehensive API data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b863acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the enhanced PR data\n",
    "df = pd.read_csv('../analysis/enhanced/enhanced_pr_data_complete.csv')\n",
    "\n",
    "print(f\"üìä Dataset loaded successfully!\")\n",
    "print(f\"üìà Total PRs: {len(df):,}\")\n",
    "print(f\"üìã Columns: {len(df.columns)}\")\n",
    "print(f\"üìÖ Date range: {df['created_at'].min()} to {df['created_at'].max()}\")\n",
    "\n",
    "# Display basic info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586c7c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and cleaning\n",
    "def clean_and_prepare_data(df):\n",
    "    \"\"\"Clean and prepare the DataFrame for analysis\"\"\"\n",
    "    \n",
    "    # Convert date columns\n",
    "    date_columns = ['created_at', 'merged_at', 'closed_at']\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # Extract date components\n",
    "    df['created_date'] = df['created_at'].dt.date\n",
    "    df['created_year_month'] = df['created_at'].dt.to_period('M')\n",
    "    df['created_weekday'] = df['created_at'].dt.day_name()\n",
    "    df['created_hour'] = df['created_at'].dt.hour\n",
    "    \n",
    "    # Clean repository names (extract just the repo name)\n",
    "    df['repo_short'] = df['repository'].str.split('/').str[-1]\n",
    "    \n",
    "    # Create PR size categories\n",
    "    df['lines_total'] = df['additions'] + df['deletions']\n",
    "    df['pr_size_category'] = pd.cut(df['lines_total'], \n",
    "                                   bins=[0, 10, 50, 200, 1000, float('inf')],\n",
    "                                   labels=['XS (<10)', 'S (10-50)', 'M (50-200)', 'L (200-1000)', 'XL (1000+)'])\n",
    "    \n",
    "    # Create review categories\n",
    "    df['review_efficiency'] = df['approvals_count'] / (df['reviews_count'] + 1)\n",
    "    \n",
    "    # Fill NaN values\n",
    "    df['time_to_merge_hours'] = df['time_to_merge_hours'].fillna(0)\n",
    "    df['description_length'] = df['description_length'].fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = clean_and_prepare_data(df)\n",
    "print(\"‚úÖ Data cleaned and prepared for analysis!\")\n",
    "\n",
    "# Display sample of cleaned data\n",
    "df[['repository', 'title', 'state', 'created_date', 'pr_size_category', 'reviews_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc4ada",
   "metadata": {},
   "source": [
    "## üìà 1. High-Level Overview & Key Metrics\n",
    "\n",
    "Let's start with the big picture of your GitHub contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9bf93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level statistics\n",
    "def display_key_metrics(df):\n",
    "    \"\"\"Display key metrics and overview statistics\"\"\"\n",
    "    \n",
    "    total_prs = len(df)\n",
    "    merged_prs = len(df[df['state'] == 'MERGED'])\n",
    "    open_prs = len(df[df['state'] == 'OPEN'])\n",
    "    \n",
    "    total_additions = df['additions'].sum()\n",
    "    total_deletions = df['deletions'].sum()\n",
    "    total_files = df['files_changed'].sum()\n",
    "    \n",
    "    avg_time_to_merge = df[df['time_to_merge_hours'] > 0]['time_to_merge_hours'].mean()\n",
    "    \n",
    "    print(\"üéØ KEY METRICS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìä Total PRs Created: {total_prs:,}\")\n",
    "    print(f\"‚úÖ Merged PRs: {merged_prs:,} ({merged_prs/total_prs*100:.1f}%)\")\n",
    "    print(f\"üîÑ Open PRs: {open_prs:,} ({open_prs/total_prs*100:.1f}%)\")\n",
    "    print(f\"‚ûï Total Lines Added: {total_additions:,}\")\n",
    "    print(f\"‚ûñ Total Lines Deleted: {total_deletions:,}\")\n",
    "    print(f\"üìÅ Total Files Changed: {total_files:,}\")\n",
    "    print(f\"‚è±Ô∏è Average Time to Merge: {avg_time_to_merge:.1f} hours\")\n",
    "    print(f\"üóìÔ∏è Analysis Period: {(df['created_at'].max() - df['created_at'].min()).days} days\")\n",
    "    print(f\"üìÖ PRs per Month (avg): {total_prs / ((df['created_at'].max() - df['created_at'].min()).days / 30.44):.1f}\")\n",
    "\n",
    "display_key_metrics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aae8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create overview visualizations\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('PR Status Distribution', 'PR Size Distribution', \n",
    "                   'Monthly PR Creation', 'Repository Distribution (Top 10)'),\n",
    "    specs=[[{\"type\": \"pie\"}, {\"type\": \"bar\"}],\n",
    "           [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]]\n",
    ")\n",
    "\n",
    "# PR Status Distribution\n",
    "status_counts = df['state'].value_counts()\n",
    "fig.add_trace(go.Pie(labels=status_counts.index, values=status_counts.values, name=\"Status\"),\n",
    "              row=1, col=1)\n",
    "\n",
    "# PR Size Distribution\n",
    "size_counts = df['pr_size_category'].value_counts()\n",
    "fig.add_trace(go.Bar(x=size_counts.index, y=size_counts.values, name=\"Size\"),\n",
    "              row=1, col=2)\n",
    "\n",
    "# Monthly PR Creation\n",
    "monthly_prs = df.groupby('created_year_month').size()\n",
    "fig.add_trace(go.Scatter(x=monthly_prs.index.astype(str), y=monthly_prs.values, \n",
    "                        mode='lines+markers', name=\"Monthly PRs\"),\n",
    "              row=2, col=1)\n",
    "\n",
    "# Top 10 Repositories\n",
    "top_repos = df['repo_short'].value_counts().head(10)\n",
    "fig.add_trace(go.Bar(x=top_repos.values, y=top_repos.index, orientation='h', name=\"Top Repos\"),\n",
    "              row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"GitHub PR Analysis - Overview Dashboard\", showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a38d53d",
   "metadata": {},
   "source": [
    "## üöÄ 2. Productivity Analysis\n",
    "\n",
    "Understanding your contribution patterns and productivity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83f284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Productivity over time analysis\n",
    "def analyze_productivity_trends(df):\n",
    "    \"\"\"Analyze productivity trends over time\"\"\"\n",
    "    \n",
    "    # Monthly productivity metrics\n",
    "    monthly_stats = df.groupby('created_year_month').agg({\n",
    "        'pr_number': 'count',\n",
    "        'additions': 'sum',\n",
    "        'deletions': 'sum',\n",
    "        'files_changed': 'sum',\n",
    "        'time_to_merge_hours': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    monthly_stats.columns = ['PRs_Created', 'Lines_Added', 'Lines_Deleted', 'Files_Changed', 'Avg_Merge_Time']\n",
    "    \n",
    "    return monthly_stats\n",
    "\n",
    "monthly_productivity = analyze_productivity_trends(df)\n",
    "print(\"üìä Monthly Productivity Summary:\")\n",
    "print(monthly_productivity.tail(10))  # Show last 10 months\n",
    "\n",
    "# Visualize productivity trends\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=1,\n",
    "    subplot_titles=('Monthly PR Creation', 'Monthly Code Changes', 'Average Merge Time'),\n",
    "    shared_xaxes=True\n",
    ")\n",
    "\n",
    "months = monthly_productivity.index.astype(str)\n",
    "\n",
    "# PRs per month\n",
    "fig.add_trace(go.Scatter(x=months, y=monthly_productivity['PRs_Created'],\n",
    "                        mode='lines+markers', name='PRs Created', line=dict(color='blue')),\n",
    "              row=1, col=1)\n",
    "\n",
    "# Code changes\n",
    "fig.add_trace(go.Scatter(x=months, y=monthly_productivity['Lines_Added'],\n",
    "                        mode='lines+markers', name='Lines Added', line=dict(color='green')),\n",
    "              row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=months, y=monthly_productivity['Lines_Deleted'],\n",
    "                        mode='lines+markers', name='Lines Deleted', line=dict(color='red')),\n",
    "              row=2, col=1)\n",
    "\n",
    "# Merge time\n",
    "fig.add_trace(go.Scatter(x=months, y=monthly_productivity['Avg_Merge_Time'],\n",
    "                        mode='lines+markers', name='Avg Merge Time (hours)', line=dict(color='orange')),\n",
    "              row=3, col=1)\n",
    "\n",
    "fig.update_layout(height=900, title_text=\"Productivity Trends Over Time\")\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e8d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekly and daily patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Day of week patterns\n",
    "weekday_counts = df['created_weekday'].value_counts()\n",
    "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weekday_counts = weekday_counts.reindex(weekday_order)\n",
    "\n",
    "axes[0,0].bar(weekday_counts.index, weekday_counts.values, color='skyblue')\n",
    "axes[0,0].set_title('PRs by Day of Week')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Hour of day patterns\n",
    "hourly_counts = df['created_hour'].value_counts().sort_index()\n",
    "axes[0,1].plot(hourly_counts.index, hourly_counts.values, marker='o', color='green')\n",
    "axes[0,1].set_title('PRs by Hour of Day')\n",
    "axes[0,1].set_xlabel('Hour')\n",
    "axes[0,1].set_ylabel('Number of PRs')\n",
    "\n",
    "# PR size distribution by repository (top 5)\n",
    "top5_repos = df['repo_short'].value_counts().head(5).index\n",
    "df_top5 = df[df['repo_short'].isin(top5_repos)]\n",
    "repo_size_data = []\n",
    "for repo in top5_repos:\n",
    "    repo_data = df_top5[df_top5['repo_short'] == repo]['lines_total'].values\n",
    "    repo_size_data.append(repo_data)\n",
    "\n",
    "axes[1,0].boxplot(repo_size_data, labels=top5_repos)\n",
    "axes[1,0].set_title('PR Size Distribution by Repository (Top 5)')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "axes[1,0].set_ylabel('Total Lines Changed')\n",
    "\n",
    "# Merge time distribution\n",
    "merge_times = df[df['time_to_merge_hours'] > 0]['time_to_merge_hours']\n",
    "axes[1,1].hist(merge_times, bins=30, alpha=0.7, color='orange')\n",
    "axes[1,1].set_title('Distribution of Merge Times')\n",
    "axes[1,1].set_xlabel('Hours to Merge')\n",
    "axes[1,1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüïê Most active day: {weekday_counts.idxmax()} ({weekday_counts.max()} PRs)\")\n",
    "print(f\"üïê Most active hour: {hourly_counts.idxmax()}:00 ({hourly_counts.max()} PRs)\")\n",
    "print(f\"‚ö° Median merge time: {merge_times.median():.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2535d45d",
   "metadata": {},
   "source": [
    "## üë• 3. Collaboration & Review Analysis\n",
    "\n",
    "Analyzing review patterns, approvals, and collaboration metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5614ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review and collaboration analysis\n",
    "def analyze_collaboration(df):\n",
    "    \"\"\"Analyze review and collaboration patterns\"\"\"\n",
    "    \n",
    "    # Review statistics\n",
    "    total_reviews = df['reviews_count'].sum()\n",
    "    total_approvals = df['approvals_count'].sum()\n",
    "    total_change_requests = df['changes_requested_count'].sum()\n",
    "    \n",
    "    print(\"üë• COLLABORATION METRICS\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"üìù Total Reviews: {total_reviews:,}\")\n",
    "    print(f\"‚úÖ Total Approvals: {total_approvals:,}\")\n",
    "    print(f\"üîÑ Change Requests: {total_change_requests:,}\")\n",
    "    print(f\"üìä Approval Rate: {total_approvals/total_reviews*100:.1f}%\" if total_reviews > 0 else \"üìä Approval Rate: N/A\")\n",
    "    \n",
    "    # PRs with different review counts\n",
    "    review_distribution = df['reviews_count'].value_counts().sort_index()\n",
    "    print(f\"\\nüìà Review Distribution:\")\n",
    "    for reviews, count in review_distribution.head(6).items():\n",
    "        print(f\"  {reviews} reviews: {count} PRs ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'total_reviews': total_reviews,\n",
    "        'approval_rate': total_approvals/total_reviews if total_reviews > 0 else 0\n",
    "    }\n",
    "\n",
    "collab_stats = analyze_collaboration(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dbe97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize collaboration patterns\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Review Count Distribution', 'Approval vs Change Requests', \n",
    "                   'Comments vs Reviews Correlation', 'Review Efficiency by Repository'),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "           [{\"type\": \"scatter\"}, {\"type\": \"box\"}]]\n",
    ")\n",
    "\n",
    "# Review count distribution\n",
    "review_dist = df['reviews_count'].value_counts().sort_index()\n",
    "fig.add_trace(go.Bar(x=review_dist.index[:10], y=review_dist.values[:10], \n",
    "                    name=\"Review Distribution\"),\n",
    "              row=1, col=1)\n",
    "\n",
    "# Approval vs Change Requests scatter\n",
    "fig.add_trace(go.Scatter(x=df['approvals_count'], y=df['changes_requested_count'],\n",
    "                        mode='markers', name=\"Approval vs Changes\",\n",
    "                        text=df['title'].str[:30], hovertemplate='%{text}<br>Approvals: %{x}<br>Changes: %{y}'),\n",
    "              row=1, col=2)\n",
    "\n",
    "# Comments vs Reviews correlation\n",
    "fig.add_trace(go.Scatter(x=df['reviews_count'], y=df['comments_count'],\n",
    "                        mode='markers', name=\"Comments vs Reviews\",\n",
    "                        text=df['title'].str[:30]),\n",
    "              row=2, col=1)\n",
    "\n",
    "# Review efficiency by top repositories\n",
    "top_repos_for_reviews = df['repo_short'].value_counts().head(5).index\n",
    "for i, repo in enumerate(top_repos_for_reviews):\n",
    "    repo_data = df[df['repo_short'] == repo]\n",
    "    efficiency = repo_data['review_efficiency']\n",
    "    fig.add_trace(go.Box(y=efficiency, name=repo, showlegend=False),\n",
    "                  row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"Collaboration & Review Analysis\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f8804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most collaborative repositories and reviewers analysis\n",
    "print(\"ü§ù MOST COLLABORATIVE REPOSITORIES:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "repo_collab = df.groupby('repo_short').agg({\n",
    "    'reviews_count': 'mean',\n",
    "    'approvals_count': 'mean',\n",
    "    'comments_count': 'mean',\n",
    "    'pr_number': 'count'\n",
    "}).round(2)\n",
    "\n",
    "repo_collab = repo_collab[repo_collab['pr_number'] >= 5]  # Only repos with 5+ PRs\n",
    "repo_collab = repo_collab.sort_values('reviews_count', ascending=False)\n",
    "\n",
    "print(repo_collab.head(10))\n",
    "\n",
    "# Analyze unique reviewers (extract from the unique_reviewers field)\n",
    "print(\"\\nüë®‚Äçüíº REVIEWER ANALYSIS:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Count total unique reviewer mentions across all PRs\n",
    "all_reviewers = []\n",
    "for reviewers_str in df['unique_reviewers'].dropna():\n",
    "    if reviewers_str and reviewers_str != '':\n",
    "        reviewers = reviewers_str.split('|')\n",
    "        all_reviewers.extend([r.strip() for r in reviewers if r.strip()])\n",
    "\n",
    "if all_reviewers:\n",
    "    reviewer_counts = pd.Series(all_reviewers).value_counts()\n",
    "    print(\"Top 10 Most Frequent Reviewers:\")\n",
    "    print(reviewer_counts.head(10))\n",
    "else:\n",
    "    print(\"No reviewer data available in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fbe1b2",
   "metadata": {},
   "source": [
    "## üìä 4. Repository Specialization Analysis\n",
    "\n",
    "Understanding your contribution patterns across different repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc75275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repository analysis\n",
    "def analyze_repositories(df):\n",
    "    \"\"\"Comprehensive repository analysis\"\"\"\n",
    "    \n",
    "    repo_stats = df.groupby('repo_short').agg({\n",
    "        'pr_number': 'count',\n",
    "        'additions': ['sum', 'mean'],\n",
    "        'deletions': ['sum', 'mean'], \n",
    "        'files_changed': ['sum', 'mean'],\n",
    "        'time_to_merge_hours': 'mean',\n",
    "        'reviews_count': 'mean',\n",
    "        'state': lambda x: (x == 'MERGED').sum() / len(x)\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names\n",
    "    repo_stats.columns = ['Total_PRs', 'Total_Additions', 'Avg_Additions',\n",
    "                         'Total_Deletions', 'Avg_Deletions', 'Total_Files', 'Avg_Files',\n",
    "                         'Avg_Merge_Time', 'Avg_Reviews', 'Merge_Rate']\n",
    "    \n",
    "    repo_stats = repo_stats.sort_values('Total_PRs', ascending=False)\n",
    "    return repo_stats\n",
    "\n",
    "repo_analysis = analyze_repositories(df)\n",
    "print(\"üèÜ TOP REPOSITORIES BY CONTRIBUTION:\")\n",
    "print(\"=\" * 50)\n",
    "print(repo_analysis.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d3593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repository specialization visualization\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Repository Contribution Distribution', 'Avg PR Size by Repository', \n",
    "                   'Merge Rate by Repository', 'Repository Activity Heatmap')\n",
    ")\n",
    "\n",
    "# Top 15 repositories by PR count\n",
    "top_repos = repo_analysis.head(15)\n",
    "\n",
    "# Contribution distribution (treemap style as bar chart)\n",
    "fig.add_trace(go.Bar(x=top_repos.index, y=top_repos['Total_PRs'],\n",
    "                    name=\"PR Count\", marker_color='lightblue'),\n",
    "              row=1, col=1)\n",
    "\n",
    "# Average PR size by repository  \n",
    "fig.add_trace(go.Scatter(x=top_repos['Avg_Additions'], y=top_repos['Avg_Deletions'],\n",
    "                        mode='markers+text', text=top_repos.index,\n",
    "                        textposition=\"top center\", name=\"Avg Size\",\n",
    "                        marker=dict(size=top_repos['Total_PRs']/2, opacity=0.6)),\n",
    "              row=1, col=2)\n",
    "\n",
    "# Merge rate by repository\n",
    "fig.add_trace(go.Bar(x=top_repos.index, y=top_repos['Merge_Rate'],\n",
    "                    name=\"Merge Rate\", marker_color='lightgreen'),\n",
    "              row=2, col=1)\n",
    "\n",
    "# Monthly activity heatmap for top 5 repos\n",
    "top_5_repos = df['repo_short'].value_counts().head(5).index\n",
    "heatmap_data = []\n",
    "months = sorted(df['created_year_month'].unique())\n",
    "\n",
    "for repo in top_5_repos:\n",
    "    repo_monthly = df[df['repo_short'] == repo].groupby('created_year_month').size()\n",
    "    repo_row = [repo_monthly.get(month, 0) for month in months]\n",
    "    heatmap_data.append(repo_row)\n",
    "\n",
    "fig.add_trace(go.Heatmap(z=heatmap_data, \n",
    "                        x=[str(m) for m in months][-12:],  # Last 12 months\n",
    "                        y=list(top_5_repos),\n",
    "                        colorscale='Blues', name=\"Activity\"),\n",
    "              row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"Repository Specialization Analysis\")\n",
    "fig.update_xaxes(tickangle=45, row=1, col=1)\n",
    "fig.update_xaxes(tickangle=45, row=2, col=1)\n",
    "fig.update_xaxes(tickangle=45, row=2, col=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65fc18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repository specialization insights\n",
    "print(\"üéØ REPOSITORY SPECIALIZATION INSIGHTS:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Identify repository types based on patterns\n",
    "repo_analysis['Impact_Score'] = (repo_analysis['Total_Additions'] + repo_analysis['Total_Deletions']) / repo_analysis['Total_PRs']\n",
    "repo_analysis['Collaboration_Score'] = repo_analysis['Avg_Reviews']\n",
    "\n",
    "# Categorize repositories\n",
    "high_volume = repo_analysis[repo_analysis['Total_PRs'] >= 50]\n",
    "high_impact = repo_analysis[repo_analysis['Impact_Score'] >= 100]\n",
    "collaborative = repo_analysis[repo_analysis['Avg_Reviews'] >= 1.5]\n",
    "\n",
    "print(f\"üìà High Volume Repos (50+ PRs): {len(high_volume)}\")\n",
    "if not high_volume.empty:\n",
    "    print(f\"   Top: {high_volume.index[0]} ({high_volume.iloc[0]['Total_PRs']:.0f} PRs)\")\n",
    "\n",
    "print(f\"üí• High Impact Repos (100+ lines/PR): {len(high_impact)}\")\n",
    "if not high_impact.empty:\n",
    "    print(f\"   Top: {high_impact.sort_values('Impact_Score', ascending=False).index[0]} ({high_impact.sort_values('Impact_Score', ascending=False).iloc[0]['Impact_Score']:.0f} lines/PR)\")\n",
    "\n",
    "print(f\"üë• Collaborative Repos (1.5+ reviews/PR): {len(collaborative)}\")\n",
    "if not collaborative.empty:\n",
    "    print(f\"   Top: {collaborative.sort_values('Avg_Reviews', ascending=False).index[0]} ({collaborative.sort_values('Avg_Reviews', ascending=False).iloc[0]['Avg_Reviews']:.1f} reviews/PR)\")\n",
    "\n",
    "# Show repository diversity\n",
    "total_repos = len(repo_analysis)\n",
    "print(f\"\\nüèóÔ∏è Total Repositories Contributed To: {total_repos}\")\n",
    "print(f\"üìä Repository Diversity Index: {1 - (repo_analysis['Total_PRs']**2).sum() / (repo_analysis['Total_PRs'].sum()**2):.3f}\")\n",
    "print(\"   (Higher = more evenly distributed across repos)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1f9cd8",
   "metadata": {},
   "source": [
    "## ‚è±Ô∏è 5. Timeline & Efficiency Analysis\n",
    "\n",
    "Understanding merge times, workflow efficiency, and temporal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf41037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline and efficiency analysis\n",
    "def analyze_timeline_efficiency(df):\n",
    "    \"\"\"Analyze timeline patterns and workflow efficiency\"\"\"\n",
    "    \n",
    "    # Filter for merged PRs with valid merge times\n",
    "    merged_df = df[(df['state'] == 'MERGED') & (df['time_to_merge_hours'] > 0)]\n",
    "    \n",
    "    if len(merged_df) == 0:\n",
    "        print(\"‚ùå No merged PRs with valid merge times found.\")\n",
    "        return\n",
    "    \n",
    "    print(\"‚è±Ô∏è TIMELINE & EFFICIENCY METRICS\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"üìä Analyzed Merged PRs: {len(merged_df):,}\")\n",
    "    print(f\"‚ö° Median Merge Time: {merged_df['time_to_merge_hours'].median():.1f} hours\")\n",
    "    print(f\"üìà Average Merge Time: {merged_df['time_to_merge_hours'].mean():.1f} hours\")\n",
    "    print(f\"üöÄ Fastest Merge: {merged_df['time_to_merge_hours'].min():.1f} hours\")\n",
    "    print(f\"üêå Slowest Merge: {merged_df['time_to_merge_hours'].max():.1f} hours\")\n",
    "    \n",
    "    # Efficiency categories\n",
    "    fast_merges = len(merged_df[merged_df['time_to_merge_hours'] <= 24])  # Same day\n",
    "    medium_merges = len(merged_df[(merged_df['time_to_merge_hours'] > 24) & \n",
    "                                 (merged_df['time_to_merge_hours'] <= 168)])  # Within a week\n",
    "    slow_merges = len(merged_df[merged_df['time_to_merge_hours'] > 168])  # Over a week\n",
    "    \n",
    "    print(f\"\\nüöÄ Fast Merges (‚â§24h): {fast_merges} ({fast_merges/len(merged_df)*100:.1f}%)\")\n",
    "    print(f\"‚ö° Medium Merges (1-7 days): {medium_merges} ({medium_merges/len(merged_df)*100:.1f}%)\")\n",
    "    print(f\"üêå Slow Merges (>7 days): {slow_merges} ({slow_merges/len(merged_df)*100:.1f}%)\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "merged_prs = analyze_timeline_efficiency(df)\n",
    "\n",
    "if merged_prs is not None and not merged_prs.empty:\n",
    "    # Merge time analysis by various factors\n",
    "    print(\"\\nüìä MERGE TIME BY FACTORS:\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # By PR size\n",
    "    size_merge_times = merged_prs.groupby('pr_size_category')['time_to_merge_hours'].median().sort_values()\n",
    "    print(\"By PR Size (median hours):\")\n",
    "    for size, time in size_merge_times.items():\n",
    "        print(f\"  {size}: {time:.1f}h\")\n",
    "    \n",
    "    # By repository (top 5)\n",
    "    top_repos_merge = merged_prs['repo_short'].value_counts().head(5).index\n",
    "    repo_merge_times = merged_prs[merged_prs['repo_short'].isin(top_repos_merge)].groupby('repo_short')['time_to_merge_hours'].median().sort_values()\n",
    "    print(f\"\\nBy Repository (top 5, median hours):\")\n",
    "    for repo, time in repo_merge_times.items():\n",
    "        print(f\"  {repo}: {time:.1f}h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09de83ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline visualization\n",
    "if merged_prs is not None and not merged_prs.empty:\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Merge Time Distribution', 'Merge Time vs PR Size', \n",
    "                       'Merge Time Trends Over Time', 'Merge Efficiency by Day of Week')\n",
    "    )\n",
    "    \n",
    "    # Merge time distribution (log scale for better visualization)\n",
    "    merge_times_hours = merged_prs['time_to_merge_hours']\n",
    "    fig.add_trace(go.Histogram(x=np.log10(merge_times_hours + 1), \n",
    "                              name=\"Log10(Hours+1)\", nbinsx=30),\n",
    "                  row=1, col=1)\n",
    "    \n",
    "    # Merge time vs PR size\n",
    "    for size_cat in merged_prs['pr_size_category'].dropna().unique():\n",
    "        size_data = merged_prs[merged_prs['pr_size_category'] == size_cat]['time_to_merge_hours']\n",
    "        fig.add_trace(go.Box(y=size_data, name=str(size_cat), showlegend=False),\n",
    "                      row=1, col=2)\n",
    "    \n",
    "    # Merge time trends over time (monthly averages)\n",
    "    monthly_merge_times = merged_prs.groupby('created_year_month')['time_to_merge_hours'].mean()\n",
    "    fig.add_trace(go.Scatter(x=monthly_merge_times.index.astype(str), \n",
    "                            y=monthly_merge_times.values,\n",
    "                            mode='lines+markers', name=\"Monthly Avg\",\n",
    "                            line=dict(color='red')),\n",
    "                  row=2, col=1)\n",
    "    \n",
    "    # Merge efficiency by day of week\n",
    "    weekday_merge_times = merged_prs.groupby('created_weekday')['time_to_merge_hours'].median()\n",
    "    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    weekday_merge_times = weekday_merge_times.reindex(weekday_order)\n",
    "    \n",
    "    fig.add_trace(go.Bar(x=weekday_merge_times.index, y=weekday_merge_times.values,\n",
    "                        name=\"Median Merge Time\", marker_color='lightcoral'),\n",
    "                  row=2, col=2)\n",
    "    \n",
    "    fig.update_layout(height=800, title_text=\"Timeline & Efficiency Analysis\")\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot create timeline visualizations - no valid merge time data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5667c1eb",
   "metadata": {},
   "source": [
    "## üéØ 6. Key Insights & Recommendations\n",
    "\n",
    "Summary of findings and actionable insights from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate key insights and recommendations\n",
    "def generate_insights(df, repo_analysis, merged_prs):\n",
    "    \"\"\"Generate key insights and recommendations based on the analysis\"\"\"\n",
    "    \n",
    "    insights = []\n",
    "    \n",
    "    # Productivity insights\n",
    "    total_prs = len(df)\n",
    "    avg_monthly_prs = total_prs / ((df['created_at'].max() - df['created_at'].min()).days / 30.44)\n",
    "    top_repo = df['repo_short'].value_counts().index[0]\n",
    "    top_repo_count = df['repo_short'].value_counts().iloc[0]\n",
    "    \n",
    "    insights.append(f\"üöÄ **High Productivity**: You've created {total_prs:,} PRs with an average of {avg_monthly_prs:.1f} PRs per month.\")\n",
    "    insights.append(f\"üéØ **Repository Focus**: Your top repository '{top_repo}' accounts for {top_repo_count} PRs ({top_repo_count/total_prs*100:.1f}% of total contributions).\")\n",
    "    \n",
    "    # Code impact insights\n",
    "    total_lines = df['additions'].sum() + df['deletions'].sum()\n",
    "    avg_pr_size = total_lines / total_prs\n",
    "    large_prs = len(df[df['lines_total'] > 200])\n",
    "    \n",
    "    insights.append(f\"üíª **Code Impact**: You've changed {total_lines:,} lines across all PRs (avg: {avg_pr_size:.0f} lines per PR).\")\n",
    "    insights.append(f\"üìè **PR Sizing**: {large_prs} PRs are large (200+ lines), representing {large_prs/total_prs*100:.1f}% of your work.\")\n",
    "    \n",
    "    # Collaboration insights\n",
    "    reviewed_prs = len(df[df['reviews_count'] > 0])\n",
    "    avg_reviews = df[df['reviews_count'] > 0]['reviews_count'].mean() if reviewed_prs > 0 else 0\n",
    "    \n",
    "    insights.append(f\"üë• **Collaboration**: {reviewed_prs} PRs received reviews ({reviewed_prs/total_prs*100:.1f}%), with an average of {avg_reviews:.1f} reviews per reviewed PR.\")\n",
    "    \n",
    "    # Efficiency insights\n",
    "    if merged_prs is not None and not merged_prs.empty:\n",
    "        fast_merge_rate = len(merged_prs[merged_prs['time_to_merge_hours'] <= 24]) / len(merged_prs) * 100\n",
    "        median_merge = merged_prs['time_to_merge_hours'].median()\n",
    "        \n",
    "        insights.append(f\"‚ö° **Merge Efficiency**: {fast_merge_rate:.1f}% of PRs merge within 24 hours, with a median merge time of {median_merge:.1f} hours.\")\n",
    "    \n",
    "    # Repository diversity\n",
    "    repo_count = len(df['repo_short'].unique())\n",
    "    insights.append(f\"üèóÔ∏è **Repository Diversity**: You contribute to {repo_count} different repositories, showing broad technical engagement.\")\n",
    "    \n",
    "    # Timing insights\n",
    "    most_active_day = df['created_weekday'].value_counts().index[0]\n",
    "    most_active_hour = df['created_hour'].value_counts().index[0]\n",
    "    insights.append(f\"üìÖ **Activity Patterns**: Most active on {most_active_day}s around {most_active_hour}:00, indicating consistent work patterns.\")\n",
    "    \n",
    "    return insights\n",
    "\n",
    "# Generate and display insights\n",
    "key_insights = generate_insights(df, repo_analysis, merged_prs)\n",
    "\n",
    "print(\"üéØ KEY INSIGHTS & FINDINGS\")\n",
    "print(\"=\" * 50)\n",
    "for i, insight in enumerate(key_insights, 1):\n",
    "    print(f\"{i}. {insight}\")\n",
    "\n",
    "print(\"\\nüí° RECOMMENDATIONS\")\n",
    "print(\"=\" * 30)\n",
    "print(\"1. üéØ **Focus Areas**: Consider consolidating efforts on fewer repositories for deeper impact\")\n",
    "print(\"2. üìè **PR Sizing**: Break down large PRs (200+ lines) for faster review cycles\")  \n",
    "print(\"3. ‚ö° **Review Speed**: Leverage your fast merge patterns to establish best practices\")\n",
    "print(\"4. üë• **Collaboration**: Maintain high review engagement to ensure code quality\")\n",
    "print(\"5. üìä **Tracking**: Use these metrics as KPIs for ongoing productivity measurement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6df84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a final summary dashboard\n",
    "summary_metrics = {\n",
    "    'Total PRs': len(df),\n",
    "    'Merged Rate': f\"{len(df[df['state'] == 'MERGED'])/len(df)*100:.1f}%\",\n",
    "    'Avg PR Size': f\"{(df['additions'] + df['deletions']).mean():.0f} lines\",\n",
    "    'Top Repository': df['repo_short'].value_counts().index[0],\n",
    "    'Total Repositories': len(df['repo_short'].unique()),\n",
    "    'Avg Reviews per PR': f\"{df['reviews_count'].mean():.1f}\",\n",
    "    'Lines of Code Changed': f\"{(df['additions'] + df['deletions']).sum():,}\",\n",
    "    'Most Active Day': df['created_weekday'].value_counts().index[0]\n",
    "}\n",
    "\n",
    "print(\"üìä FINAL SUMMARY DASHBOARD\")\n",
    "print(\"=\" * 50)\n",
    "for metric, value in summary_metrics.items():\n",
    "    print(f\"üìà {metric}: {value}\")\n",
    "\n",
    "print(f\"\\nüéâ Analysis Complete! This comprehensive analysis covers {len(df):,} PRs across {len(df['repo_short'].unique())} repositories.\")\n",
    "print(\"üíæ All visualizations and metrics are available for further exploration and sharing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108db403",
   "metadata": {},
   "source": [
    "## üìù Next Steps\n",
    "\n",
    "This analysis provides a comprehensive overview of your GitHub contributions. You can:\n",
    "\n",
    "1. **Deep Dive**: Focus on specific repositories or time periods for detailed analysis\n",
    "2. **Comparison**: Compare your metrics against team averages or industry benchmarks  \n",
    "3. **Optimization**: Use insights to optimize your workflow and collaboration patterns\n",
    "4. **Reporting**: Export key visualizations for presentations or performance reviews\n",
    "5. **Monitoring**: Set up regular analysis to track changes in your contribution patterns\n",
    "\n",
    "### üîÑ Refreshing the Analysis\n",
    "To update this analysis with new data:\n",
    "1. Re-run the PR enhancement script: `cd analysis/tools && python3 enhance_pr_data.py`\n",
    "2. Restart this notebook and run all cells with the updated data\n",
    "\n",
    "---\n",
    "*Analysis generated on February 21, 2026 | Data source: GitHub CLI enhanced extraction*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
